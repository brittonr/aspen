# ASPEN CODEBASE COMPREHENSIVE AUDIT REPORT

Date: December 9, 2025
Codebase: Aspen - Distributed Orchestration Layer
Total Lines of Code: 12,187 (main modules)
Language: Rust 2024 Edition
Build System: Nix with flakes

==============================================================================
EXECUTIVE SUMMARY
==============================================================================

Aspen is a foundational orchestration layer for distributed systems built on Rust. 
The codebase implements a complete Raft consensus engine integrated with actor-based 
concurrency (ractor), P2P networking (Iroh), and deterministic testing (madsim). 

Key Architectural Principles:
- Trait-based Abstraction: Tight ClusterController and KeyValueStore interfaces
- Tiger Style Compliance: Fixed limits, explicit types, fail-fast semantics
- Vendored OpenRaft: v0.10.0 for independent control of consensus layer
- Hybrid Storage: redb for logs, SQLite for state machine
- Actor Supervision: Multi-level restart and health monitoring
- P2P Transport: Iroh QUIC with gossip-based peer discovery

==============================================================================
1. API MODULE (src/api/) - 368 LOC
==============================================================================

PURPOSE:
Defines narrow trait-based interfaces for control plane and KV operations,
with in-memory mock implementations for testing.

KEY TRAITS:

1. ClusterController
   - init(InitRequest) -> Result<ClusterState>
   - add_learner(AddLearnerRequest) -> Result<ClusterState>
   - change_membership(ChangeMembershipRequest) -> Result<ClusterState>
   - current_state() -> Result<ClusterState>
   - get_metrics() -> Result<RaftMetrics>
   - trigger_snapshot() -> Result<Option<LogId>>
   - get_leader() -> Result<Option<u64>>

2. KeyValueStore
   - write(WriteRequest) -> Result<WriteResult>
   - read(ReadRequest) -> Result<ReadResult>
   - delete(DeleteRequest) -> Result<DeleteResult>

KEY TYPES:
- ClusterNode: id (u64), addr (String), raft_addr (Option<String>)
- ClusterState: nodes, members, learners
- WriteCommand: Set, SetMulti, Delete, DeleteMulti
- ReadRequest/ReadResult: key/value pairs
- DeleteRequest/DeleteResult: key and deletion success flag

ERROR TYPES:
- ControlPlaneError: InvalidRequest, NotInitialized, Failed
- KeyValueStoreError: NotFound, KeyTooLarge, ValueTooLarge, BatchTooLarge, Timeout

IMPLEMENTATIONS:
- RaftControlClient (real Raft backend)
- DeterministicClusterController (in-memory mock)
- KvClient (Raft-backed)
- DeterministicKeyValueStore (in-memory mock)

==============================================================================
2. RAFT MODULE (src/raft/) - ~8,500 LOC, 13 files
==============================================================================

CORE: RaftActor (mod.rs, 687 LOC)

The main actor driving Raft consensus through ractor message passing.

Configuration:
- node_id: Raft node identifier
- cluster_config: OpenRaft configuration
- log_store: Log storage backend
- state_machine: Replicated state machine
- network_factory: IRPC network for peer communication
- state_machine_variant: Enum selecting active state machine (InMemory/Redb/Sqlite)

Messages Handled:
- Init, AddLearner, ChangeMembership (cluster operations)
- Write, Read, Delete (KV operations)
- GetMetrics, GetState, TriggerSnapshot (observability)

STORAGE LAYER (storage.rs 948 LOC, storage_sqlite.rs 1,088 LOC)

StorageBackend Enum:
- InMemory: BTreeMap-based, non-durable (testing)
- Redb: Persistent append-only log (deprecated)
- Sqlite: Persistent SQLite state machine (recommended)

SqliteStateMachine:
- Connection pooling: r2d2 with 10 read connections + 1 write connection
- WAL mode enabled for concurrency
- Batch operations: SetMulti up to 100 keys in transaction
- Snapshot support with bounded memory
- Schema: kv_data (key/value), snapshot_meta (metadata)

Error Handling (snafu):
- OpenDatabase, Execute, Query, Serialize, Deserialize
- JsonSerialize, JsonDeserialize, IoError
- DiskSpaceInsufficient (>95% usage)

NETWORK LAYER (network.rs 408 LOC, server.rs 262 LOC)

IrpcRaftNetworkFactory:
- Implements RaftNetworkFactory using IRPC over Iroh
- peer_addrs: HashMap<NodeId, EndpointAddr> (bounded to MAX_PEERS=1000)
- failure_detector: NodeFailureDetector for crash detection
- Dynamic peer addition during runtime

RaftRpcServer:
- Listens on Iroh endpoint for incoming IRPC connections
- Enforces concurrency limits:
  - MAX_CONCURRENT_CONNECTIONS: 500 total
  - MAX_STREAMS_PER_CONNECTION: 100 per peer
- Deserializes RaftRpcProtocol and forwards to Raft core

RPC Protocol (rpc.rs):
- Vote: Leader election votes (oneshot)
- AppendEntries: Log replication and heartbeats (oneshot)
- InstallSnapshot: Full snapshot transfer (oneshot)

NETWORK TIMEOUTS:
- IROH_CONNECT_TIMEOUT: 5 seconds
- IROH_STREAM_OPEN_TIMEOUT: 2 seconds
- IROH_READ_TIMEOUT: 10 seconds (accounts for snapshots)
- MAX_RPC_MESSAGE_SIZE: 10 MB
- MAX_SNAPSHOT_SIZE: 100 MB

SUPERVISION (supervision.rs, 1,580 LOC)

Three-Phase Health Monitoring:

1. Health Checks (default every 5s):
   - Ping RaftActor with 25ms timeout
   - Track consecutive failures

2. Automatic Restart (on 3+ failures):
   - Exponential backoff: 1s, 2s, 4s, 8s, 16s (capped)
   - Storage validation before restart
   - Meltdown detection to prevent loops

3. Circuit Breaker:
   - Closed: Normal, restarts allowed
   - Open: Meltdown detected, restarts blocked (300s wait)
   - HalfOpen: Test recovery with single restart

Configuration:
- max_restarts_per_window: 3
- restart_window_secs: 600 (10 minutes)
- actor_stability_duration_secs: 300 (5 min uptime required)
- health_check_interval_ms: 5000
- health_check_timeout_ms: 25
- circuit_open_duration_secs: 300
- half_open_stability_duration_secs: 120

UTILITY COMPONENTS:

BoundedMailboxProxy (bounded_proxy.rs, 716 LOC):
- Enforces backpressure on RaftActor message queue
- Capacity: default 1000, max 10000
- Fail-fast: Returns error if full
- Metrics: messages_sent_total, messages_rejected_total, send_errors_total

NodeFailureDetector (node_failure_detection.rs, 577 LOC):
- Distinguishes actor crashes from network failures
- Connection status: Reachable, Unreachable, Unknown
- Failure classification: Actor, Network, Unknown
- Tracks up to MAX_UNREACHABLE_NODES = 1000

Learner Promotion (learner_promotion.rs, 549 LOC):
- Safe operator-controlled promotion to voters
- Safety checks:
  - Learner health (via failure detector)
  - Log catchup (<100 entries behind leader)
  - Quorum preservation
  - Max voters limit (100)
  - Membership cooldown (300s between changes)

CONSTANTS (constants.rs, 244 LOC)

Tiger Style: All limits fixed at compile time.

Network:
- MAX_RPC_MESSAGE_SIZE: 10 MB
- IROH_CONNECT_TIMEOUT: 5s
- IROH_STREAM_OPEN_TIMEOUT: 2s
- IROH_READ_TIMEOUT: 10s
- MAX_SNAPSHOT_SIZE: 100 MB

Failure Detection:
- MAX_UNREACHABLE_NODES: 1000

Storage:
- MAX_BATCH_SIZE: 1000 entries
- MAX_SETMULTI_KEYS: 100 keys
- MAX_KEY_SIZE: 1 KB
- MAX_VALUE_SIZE: 1 MB
- MAX_SNAPSHOT_ENTRIES: 1,000,000
- DEFAULT_READ_POOL_SIZE: 10 connections

Concurrency:
- MAX_STREAMS_PER_CONNECTION: 100
- MAX_CONCURRENT_CONNECTIONS: 500
- MAX_PEERS: 1000
- DEFAULT_CAPACITY: 1000 (proxy queue)
- MAX_CAPACITY: 10,000 (proxy queue)
- MAX_CONNECTIONS_PER_NODE: 100 (madsim)

Cluster:
- MAX_VOTERS: 100 nodes
- LEARNER_LAG_THRESHOLD: 100 entries
- MEMBERSHIP_COOLDOWN: 300 seconds

Supervision:
- MAX_RESTART_HISTORY_SIZE: 100
- MAX_BACKOFF_SECONDS: 16s

==============================================================================
3. CLUSTER MODULE (src/cluster/) - ~2,200 LOC, 6 files
==============================================================================

BOOTSTRAP ORCHESTRATION (bootstrap.rs, 876 LOC)

BootstrapHandle: Contains all resources for running a node
- config: ClusterBootstrapConfig
- metadata_store: Arc<MetadataStore> (node registry)
- iroh_manager: Arc<IrohEndpointManager> (P2P endpoint)
- node_server: NodeServerHandle (ractor coordination)
- raft_actor: ActorRef<RaftActorMessage>
- raft_supervisor: ActorRef<SupervisorMessage>
- raft_core: openraft::Raft<AppTypeConfig>
- state_machine: StateMachineVariant
- rpc_server: RaftRpcServer
- network_factory: Arc<IrpcRaftNetworkFactory>
- gossip_discovery: Option<GossipPeerDiscovery>
- health_monitor: Option<Arc<HealthMonitor>>

Bootstrap Sequence:
1. Load configuration (env < TOML < CLI)
2. Initialize metadata store (redb)
3. Create Iroh P2P endpoint
4. Initialize storage backends
5. Start ractor NodeServer
6. Create RaftActor with supervision
7. Register RPC endpoints
8. Start gossip discovery
9. Return BootstrapHandle

Shutdown Sequence (reverse order):
1. Stop gossip discovery
2. Shutdown IRPC server
3. Shutdown Iroh endpoint
4. Shutdown NodeServer
5. Stop RaftActor (via supervisor)
6. Wait for supervisor completion

CONFIGURATION (config.rs, 817 LOC)

ClusterBootstrapConfig: Top-level node configuration

Core Fields:
- node_id: u64 (logical Raft ID)
- data_dir: Option<PathBuf> (default: "./data/node-{id}")
- storage_backend: StorageBackend (default: Sqlite)
- host: String (default: "localhost")
- ractor_port: u16 (0 = OS-assigned)
- cookie: String (default: "aspen-cluster")
- http_addr: SocketAddr (default: 127.0.0.1:8080)
- heartbeat_interval_ms: u64 (default: 1000)
- election_timeout_min_ms: u64 (default: 3000)
- election_timeout_max_ms: u64 (default: 6000)
- iroh: IrohConfig (networking options)
- peers: Vec<String> (manual peer addresses)
- supervision_config: SupervisionConfig
- raft_mailbox_capacity: u32 (default: 1000)

Configuration Layers (precedence):
1. Environment Variables: ASPEN_NODE_ID, ASPEN_DATA_DIR, etc.
2. TOML Config File: --config /path/to/config.toml
3. Command-Line Arguments: --node-id 1 --http-addr 127.0.0.1:8080

ControlBackend Enum:
- Raft (only variant)

IrohConfig:
- bind_port: u16 (0 = OS-assigned)
- relay_mode: Custom, Default, or Disabled
- relay_url: Optional relay server
- enable_mdns: bool (LAN discovery)
- enable_gossip: bool (peer announcement)
- enable_dns_discovery: bool (DNS service lookup)
- enable_pkarr: bool (DHT discovery)
- ticket: Optional cluster join ticket

IROH ENDPOINT MANAGEMENT (mod.rs, 652 LOC)

IrohEndpointManager: Wraps Iroh endpoint and discovery services

Methods:
- new(config: IrohEndpointConfig) -> Result<Self>
- endpoint() -> &Endpoint
- node_addr() -> &EndpointAddr
- add_service<S: Service>(service: S) -> Result<ServiceHandle>
- shutdown(self) -> Result<()>

Managed Services:
- IRPC: Raft RPC protocol handler
- Gossip: Peer discovery (if enabled)
- HTTP: Control plane API

Peer Discovery Flow (with Gossip):
1. Iroh establishes connectivity (mDNS/DNS/Pkarr/manual)
2. Nodes subscribe to gossip topic (from cluster cookie hash)
3. Each node broadcasts: node_id + EndpointAddr every 10s
4. Announcements update Raft network factory peer map
5. Raft RPCs can then flow to discovered peers

METADATA STORE (metadata.rs, 492 LOC)

MetadataStore: Persistent node registry using redb

Schema:
- Table: node_metadata
- Key: u64 (node_id)
- Value: bincode(NodeMetadata)

NodeMetadata Fields:
- node_id: u64
- endpoint_id: String (hex-encoded Iroh endpoint ID)
- raft_addr: String
- status: NodeStatus
- last_updated_secs: u64

NodeStatus Enum:
- Starting: Startup phase
- Online: Healthy and responding
- Offline: Temporarily unavailable
- Removed: Permanently removed

CRUD Operations:
- register_node(metadata: NodeMetadata) -> Result<()>
- get_node(node_id: u64) -> Result<Option<NodeMetadata>>
- list_nodes() -> Result<Vec<NodeMetadata>>
- update_status(node_id: u64, status: NodeStatus) -> Result<()>
- delete_node(node_id: u64) -> Result<()>

GOSSIP DISCOVERY (gossip_discovery.rs, 406 LOC)

GossipPeerDiscovery: Broadcasts and receives metadata via iroh-gossip

Message Format:
- GossipPeerAnnouncement:
  - node_id: u64
  - endpoint_addr: EndpointAddr
  - timestamp_secs: u64

Topic Derivation:
- Topic = SHA256(cluster_cookie) first 32 bytes

Broadcast Interval: 10 seconds per node

Receiver:
- Subscribes to gossip topic
- Updates network factory peer map
- Filters stale announcements (>30 seconds)

CLUSTER TICKETS (ticket.rs, 281 LOC)

AspenClusterTicket: Bootstrap information for joining clusters

Format: aspen{base64(serialized_data)}

Contains:
- Cluster cookie (for gossip topic)
- Relay URL (NAT traversal)
- Bootstrap peers (seed addresses)
- Iroh config (discovery settings)

Usage Flow:
1. Existing node: GET /cluster-ticket -> returns ticket
2. New node: aspen-node --ticket "aspen{...}" -> joins

==============================================================================
4. KEY-VALUE SERVICE MODULE (src/kv/) - ~120 LOC
==============================================================================

KvServiceBuilder: Fluent builder for node bootstrap

Methods:
- new(node_id: NodeId, data_dir: impl Into<PathBuf>) -> Self
- with_storage(backend: StorageBackend) -> Self
- with_peers(peers: Vec<String>) -> Self
- with_gossip(enable: bool) -> Self
- with_mdns(enable: bool) -> Self
- with_heartbeat_interval_ms(interval_ms: u64) -> Self
- with_election_timeout_ms(min_ms: u64, max_ms: u64) -> Self
- start() -> Result<KvService>

Example Usage:
```
let service = KvServiceBuilder::new(1, "./data/node-1")
    .with_storage(StorageBackend::Sqlite)
    .with_gossip(true)
    .start()
    .await?;
```

KvService: Returned by KvServiceBuilder::start()

Methods:
- node_id() -> NodeId
- data_dir() -> PathBuf
- endpoint_addr() -> EndpointAddr
- raft_core() -> &openraft::Raft
- handle() -> &BootstrapHandle
- client() -> KvClient
- shutdown(self) -> Result<()>

KvClient (client.rs): Implements KeyValueStore trait

Wraps: ActorRef<RaftActorMessage>
Delegates: Operations via ractor::call_t! macro
Timeout: Default 500ms (configurable)

NodeId Type (types.rs): Newtype wrapper around u64

Purpose: Prevent accidental mixing with other u64 values
Conversions: From<u64>, From<NodeId>, FromStr, Display

==============================================================================
5. BINARY ENTRY POINT (src/bin/aspen-node.rs) - 66 KB
==============================================================================

Purpose: Production cluster node with HTTP API

CLI Arguments (clap):
- --config: TOML configuration file
- --node-id, --data-dir, --storage-backend
- --raft-addr, --http-addr, --cookie
- Iroh/networking options

HTTP Server (Axum):

Control Plane Endpoints:
- POST /cluster/init: Initialize with initial members
- POST /cluster/add-learner: Add learner node
- POST /cluster/change-membership: Promote learners

Key-Value Endpoints:
- POST /kv/read: Read key (linearizable)
- POST /kv/write: Write key-value (replicated)
- POST /kv/delete: Delete key (replicated)

Monitoring Endpoints:
- GET /health: Health check with supervision status
- GET /metrics: Prometheus metrics
- GET /cluster-ticket: Return cluster join ticket
- GET /raft-metrics: OpenRaft metrics JSON

Admin Endpoints:
- POST /admin/promote-learner: Manual promotion

Graceful Shutdown:
- Handles SIGTERM/SIGINT
- Coordinates shutdown of all components

==============================================================================
6. UTILITIES & TESTING
==============================================================================

Utils Module (utils.rs):

Disk Space Checking (Tiger Style):
- check_disk_space(path: &Path) -> Result<DiskSpace>
- ensure_disk_space_available(path: &Path) -> Result<()>

Fixed Threshold: 95% disk usage
Platform Support: Unix (libc::statvfs), Windows (not yet)

Testing Infrastructure (testing/):

AspenRouter:
- In-memory Raft cluster simulator
- Deterministic networking for multi-node tests
- Integration with madsim for deterministic simulation

madsim Support:
- Deterministic time advancement
- Controlled packet loss/delays
- Deterministic event ordering

==============================================================================
MODULE DEPENDENCIES
==============================================================================

Trait Implementations:
- ClusterController: implemented by RaftControlClient, DeterministicClusterController
- KeyValueStore: implemented by KvClient, DeterministicKeyValueStore

File Dependencies:
lib.rs
├─ api/: Trait definitions
│  ├─ mod.rs: ClusterController, KeyValueStore, types
│  └─ inmemory.rs: Mock implementations (DeterministicClusterController, DeterministicKeyValueStore)
├─ raft/: Consensus engine
│  ├─ mod.rs: RaftActor, RaftControlClient (implements ClusterController + KeyValueStore)
│  ├─ storage.rs: Backend abstraction
│  ├─ storage_sqlite.rs: SQLite state machine (1,088 LOC)
│  ├─ network.rs: IRPC network layer (408 LOC)
│  ├─ server.rs: IRPC server for incoming RPCs (262 LOC)
│  ├─ supervision.rs: RaftSupervisor + health (1,580 LOC)
│  ├─ learner_promotion.rs: Membership changes (549 LOC)
│  ├─ bounded_proxy.rs: Mailbox backpressure (716 LOC)
│  ├─ node_failure_detection.rs: Crash detection (577 LOC)
│  ├─ madsim_network.rs: Deterministic network (496 LOC)
│  ├─ constants.rs: Tiger Style limits (244 LOC)
│  ├─ types.rs: OpenRaft type config (76 LOC)
│  ├─ rpc.rs: IRPC protocol (100 LOC)
│  └─ storage_validation.rs: Pre-restart checks (564 LOC)
├─ cluster/: Coordination
│  ├─ mod.rs: IrohEndpointManager, NodeServerConfig (652 LOC)
│  ├─ bootstrap.rs: Node startup (876 LOC)
│  ├─ config.rs: Configuration types (817 LOC)
│  ├─ metadata.rs: Node registry (492 LOC)
│  ├─ gossip_discovery.rs: Peer discovery (406 LOC)
│  └─ ticket.rs: Join tickets (281 LOC)
├─ kv/: High-level API
│  ├─ mod.rs: KvServiceBuilder, KvService (calls cluster::bootstrap)
│  ├─ client.rs: KvClient (implements KeyValueStore)
│  └─ types.rs: NodeId newtype
├─ testing/: Integration tests
│  └─ router.rs: AspenRouter for in-memory tests
└─ utils.rs: Disk space checks

bin/aspen-node.rs:
├─ api::*: Traits + types
├─ cluster::bootstrap: Node initialization
├─ raft::*: RaftControlClient, RaftActor
├─ kv::KvClient: Client API
└─ Axum: REST API server

==============================================================================
ERROR HANDLING STRATEGY
==============================================================================

Library Layer (raft/, cluster/, api/):
- Use snafu for explicit, contextual errors
- Typed error enums per module
- Include file paths, buffer sizes, timestamps

Application Layer (bin/aspen-node.rs):
- Use anyhow for general errors
- HTTP responses with semantic status codes
- JSON error bodies with human-readable messages

Example Storage Errors (snafu):
- OpenDatabase: Failed to open/create DB
- Execute: SQL execution failed
- Query: Query failed
- Serialize/Deserialize: Binary encoding
- JsonSerialize/JsonDeserialize: JSON encoding
- DiskSpaceInsufficient: >95% usage
- IoError: Filesystem I/O issues

Example HTTP Responses:
- 200 OK: Operation succeeded
- 400 Bad Request: Validation failed (KeyTooLarge, ValueTooLarge, etc.)
- 404 Not Found: Key not found (idempotent)
- 408 Request Timeout: Operation exceeded timeout
- 500 Internal Server Error: Unexpected failure

==============================================================================
TIGER STYLE COMPLIANCE SUMMARY
==============================================================================

Principle                | Implementation
-------------------------|----------------------------------------------
Explicit Types           | u64 for node IDs (not usize), Duration for timeouts, NodeId newtype
Fixed Limits            | MAX_BATCH_SIZE, MAX_SNAPSHOT_SIZE, MAX_PEERS, connection pools
Static Allocation       | All limits in constants.rs at compile time
Variable Scope          | Variables declared near usage, minimize lifetime
Assertions              | Pre/post-condition checks in RaftActor transitions
Fail Fast               | Disk checks before writes, config validation before startup
Control Flow            | Supervisor handles restarts, Result<T> error propagation
Function Size           | Core handlers <70 lines, complex logic split into helpers
Network Optimization    | IRPC for low-latency, batch operations, connection pooling
Disk Optimization       | Append-only log (redb), SQLite WAL, snapshot dedup
Memory Optimization     | Arc for shared ownership, bounded queues, connection pools

==============================================================================
HEALTH & MONITORING ENDPOINTS
==============================================================================

GET /health Response:
{
  "node_id": 1,
  "state": "Leader|Follower|Learner|Candidate",
  "current_leader": 1,
  "term": 42,
  "supervision": {
    "status": "healthy|degraded|unhealthy",
    "consecutive_failures": 0,
    "circuit_state": "Closed|Open|HalfOpen",
    "enabled": true
  }
}

Observable Components:
1. RaftActor: Messages sent/rejected, operation latency, responsiveness
2. Supervision: Restart count, meltdown detection, circuit breaker state
3. Storage: Write latency (disk I/O), read latency (query), snapshot size
4. Network: RPC latency by peer, connection establishment, failure detection

==============================================================================
CONFIGURATION EXAMPLE
==============================================================================

# Environment (lowest priority)
export ASPEN_NODE_ID=2

# TOML file (middle priority)
cat > config.toml << EOF
node_id = 1
storage_backend = "sqlite"
EOF

# CLI (highest priority)
aspen-node --config config.toml --node-id 3

# Result: node_id=3 (CLI wins), storage=sqlite (from TOML)

==============================================================================
KEY DESIGN DECISIONS & TRADEOFFS
==============================================================================

Decision              | Rationale                    | Tradeoff
---------------------|------------------------------|------------------------------------------
Vendored OpenRaft     | Tight control, rapid iter   | Maintenance burden, manual syncs
Hybrid Storage        | Redb logs, SQLite SM        | Complexity vs single backend
Circuit Breaker       | Prevent infinite restarts    | Requires manual intervention
Backpressure         | Fail-fast over blocking      | Dropped messages vs queuing
Fixed Limits         | Predictable, DoS prevention  | Can't exceed for unusual workloads
Actor-based          | Isolation, fault tolerance   | Overhead vs raw async/threads
IRPC over HTTP       | Low-latency, binary          | Not browser-accessible
Gossip Discovery     | Automatic, resilient         | Not suitable for all topologies

==============================================================================
ENTRY POINTS SUMMARY
==============================================================================

Entry Point               | Purpose                      | Configuration
--------------------------|------------------------------|------------------------------
aspen-node binary         | Production cluster node      | TOML + CLI args
KvServiceBuilder          | Programmatic bootstrap       | Rust builder API
bootstrap_node() fn       | Low-level bootstrap          | ClusterBootstrapConfig struct
DeterministicController   | Mock control plane           | In-memory Arc<Mutex>
AspenRouter               | In-memory test harness       | Simulated networking

==============================================================================
CONCLUSION
==============================================================================

Aspen is a well-architected distributed systems framework emphasizing:

1. Safety Through Types: Explicit types prevent bugs at compile time
2. Predictability Through Limits: Fixed bounds ensure deterministic behavior
3. Resilience Through Supervision: Multi-layer health monitoring and recovery
4. Flexibility Through Traits: Narrow interfaces enable testing and pluggable impls
5. Observability Through Metrics: Health endpoints, supervision tracking, RPC metrics

The codebase balances production-readiness with testability, using madsim for
deterministic testing and in-memory implementations for unit testing.

Total Implementation Size: 12,200 LOC across 30+ modules with clear separation
of concerns and minimal cross-module coupling.

Key Technical Achievements:
- Fully functional Raft consensus engine from scratch
- Multi-level actor supervision with circuit breaker pattern
- Hybrid storage backend with connection pooling
- IRPC-based P2P network layer with failure detection
- Deterministic testing infrastructure with madsim
- Health monitoring and observability endpoints
- Configuration management with multi-layer precedence
- Cluster ticket system for easy node joining

