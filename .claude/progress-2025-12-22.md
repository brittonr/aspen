# Aspen Feature Gap Progress Tracker

**Created**: 2025-12-22
**Last Updated**: 2025-12-22 (ULTRA MODE Analysis)
**Status**: Implementation In Progress - Core Infrastructure Complete
**Target**: Close feature gaps with FoundationDB and etcd
**Final Location**: `.claude/progress-2025-12-22.md`

---

## ULTRA MODE Analysis Summary

Deep analysis conducted with parallel subagents revealed that **significant implementation work has already been completed**. The original planning document underestimated current progress.

### Key Findings

1. **P0 (Transaction Model)**: ~85% complete - OCC validation fully implemented in SQLite state machine
2. **P1 (Sharding)**: ~70% complete - Full sharding infrastructure exists in `src/sharding/`
3. **Recent Commit**: `98a88a67` (Dec 22, 2025) added 1,999 lines implementing OCC + sharding foundation

---

## Overview

This document tracks progress on the three HIGH severity gaps identified in the gap analysis.

**Implementation Strategy**: P0 (Transaction Model) and P1 (Sharding) developed **in parallel**.

| Priority | Gap | Current State | Target State | Status |
|----------|-----|---------------|--------------|--------|
| P0 | Transaction Model | OCC implemented, version tracking ~95% | True MVCC with OCC conflict detection | **85% Complete** |
| P1 | Horizontal Scaling | ShardRouter + ShardedKVStore complete | Multi-shard architecture | **70% Complete** |
| P2 | Layer Architecture | No abstraction layer | FoundationDB-style extensible layers | Blocked on P0 |

---

## P0: Transaction Model Enhancement

### Current State Analysis (UPDATED 2025-12-22)

**What's ALREADY IMPLEMENTED:**

1. **OptimisticTransaction WriteCommand** (`src/api/mod.rs:520-563`)
   - `read_set: Vec<(String, i64)>` - keys with expected versions
   - `write_set: Vec<WriteOp>` - Set/Delete operations
   - Full validation in `validate_write_command()` (lines 1238-1271)

2. **TransactionBuilder Client API** (`src/client/transaction.rs`, 284 lines)
   - Fluent builder: `.read()`, `.read_many()`, `.set()`, `.delete()`, `.execute()`
   - Full documentation and unit tests
   - Integrates with KeyValueStore trait

3. **OCC Conflict Detection** (`src/raft/storage_sqlite.rs:2610-2703`)
   - Phase 1: Validates all read_set versions against current state
   - Phase 2: Applies write_set with proper version tracking
   - Returns detailed conflict info: `conflict_key`, `expected_version`, `actual_version`

4. **Version Tracking Schema** (SQLite)
   - `version INTEGER NOT NULL` - per-key counter (1, 2, 3...)
   - `create_revision INTEGER NOT NULL` - Raft log index at creation
   - `mod_revision INTEGER NOT NULL` - Raft log index at last modification
   - Indexes: `idx_kv_mod_revision`, `idx_kv_create_revision`

5. **AppResponse OCC Fields** (`src/raft/types.rs:510-524`)
   - `occ_conflict: Option<bool>`
   - `conflict_key: Option<String>`
   - `conflict_expected_version: Option<i64>`
   - `conflict_actual_version: Option<i64>`

**What's COMPLETE:**

- [x] `apply_set()` - properly increments version (lines 2107-2134)
- [x] `apply_set_multi()` - properly increments version (lines 2155-2215)
- [x] `apply_batch()` Set operations - track version (lines 2464-2471)
- [x] `apply_conditional_batch()` Set operations - track version (lines 2570-2577)
- [x] `apply_optimistic_transaction()` - full OCC validation + version tracking

**Remaining Gaps (Minor):**

1. **In-Memory Test Backend** (`src/api/inmemory.rs:488-509`)
   - Doesn't implement OCC version tracking
   - Always returns `occ_conflict: Some(false)`
   - Impact: Tests with in-memory backend won't detect conflicts

2. **Delete Operations** - Don't preserve version history (acceptable per etcd semantics)

### Updated Implementation Phases

#### Phase 1.1: Complete Version Tracking - COMPLETE

- [x] `apply_set()` increments version (storage_sqlite.rs:2125-2128)
- [x] `apply_set_multi()` increments version (storage_sqlite.rs:2191-2193)
- [x] `apply_conditional_batch()` tracks version (storage_sqlite.rs:2570-2577)
- [x] Schema includes version, create_revision, mod_revision columns

#### Phase 1.2: Read/Write Set Capture - COMPLETE

- [x] `OptimisticTransaction` WriteCommand variant exists (api/mod.rs:520-563)
- [x] `TransactionBuilder` client API exists (client/transaction.rs)
- [x] Read set captures (key, expected_version) pairs
- [x] Write set captures WriteOp::Set and WriteOp::Delete

#### Phase 1.3: OCC Conflict Detection - COMPLETE

- [x] Conflict validation in `apply_optimistic_transaction()` (lines 2632-2654)
- [x] Detailed conflict error responses with key and version info
- [ ] Automatic retry with backoff (client responsibility, not implemented)
- [ ] In-memory backend validation (needs work)

#### Phase 1.4: Client-Side Transaction Builder - COMPLETE

- [x] `TransactionBuilder` with fluent API (client/transaction.rs:35-110)
- [x] `.read()`, `.read_many()`, `.set()`, `.delete()`, `.execute()` methods
- [x] Integration with KeyValueStore trait
- [ ] Read range support (not yet implemented)
- [ ] Documentation examples in crate docs

### Key Files (Already Modified)

- `src/raft/storage_sqlite.rs` (4,457 lines) - OCC validation implemented
- `src/api/mod.rs` (1,730 lines) - WriteCommand::OptimisticTransaction exists
- `src/client/transaction.rs` (284 lines) - TransactionBuilder complete
- `src/raft/types.rs` (909 lines) - AppRequest/AppResponse OCC fields

### Remaining Work for P0

| Task | Priority | Effort | Status |
|------|----------|--------|--------|
| In-memory backend OCC validation | Medium | 2-3 hours | Not started |
| Automatic retry with backoff | Low | 4-6 hours | Not started |
| Read range support in TransactionBuilder | Low | 2-3 hours | Not started |
| Documentation and examples | Low | 1-2 hours | Not started |

### Success Criteria (Updated)

- [x] All write operations update version/revision metadata
- [x] Conflicting concurrent transactions are detected and rejected
- [x] Client can specify read conflict keys (ranges not yet supported)
- [ ] In-memory test backend validates conflicts
- [ ] Benchmark: < 5% overhead vs current CAS operations

---

## P1: Horizontal Scaling (Sharding)

### Current State Analysis (UPDATED 2025-12-22)

**What's ALREADY IMPLEMENTED:**

1. **Jump Consistent Hash** (`src/sharding/consistent_hash.rs`, 224 lines)
   - Google's 2014 algorithm - O(log n), perfect balance
   - `JumpHash::hash(key, num_buckets) -> ShardId`
   - Minimal redistribution: 1/n keys move when adding a shard
   - Full test coverage for determinism and distribution

2. **ShardRouter** (`src/sharding/router.rs`, 350 lines)
   - `get_shard_for_key(&self, key: &str) -> ShardId`
   - `get_shards_for_prefix()` - returns all shards for prefix scans
   - `is_local_shard()` - check if shard is hosted locally
   - `add_local_shard()` / `remove_local_shard()` management

3. **ShardedKeyValueStore** (`src/sharding/sharded.rs`, 479 lines)
   - Implements `KeyValueStore` trait (wraps N shard backends)
   - Routes writes to appropriate shard by key
   - Aggregates scans across all shards with merge
   - Validates single-shard constraint for batch/transaction ops

4. **Constants** (`src/sharding/mod.rs`)
   - `MAX_SHARDS = 256` (Tiger Style bound)
   - `MIN_SHARDS = 1`
   - `DEFAULT_SHARDS = 4`

**Sharding Module Structure (1,116 lines total):**

```
src/sharding/
  mod.rs              (63 lines)  - Module exports, constants
  consistent_hash.rs  (224 lines) - Jump hash algorithm
  router.rs           (350 lines) - Key-to-shard routing
  sharded.rs          (479 lines) - ShardedKeyValueStore wrapper
```

### Updated Implementation Phases

#### Phase 2.1: Shard Router Design - COMPLETE

- [x] Jump consistent hash algorithm implemented
- [x] ShardRouter struct with key-to-shard mapping
- [x] Unit tests for routing and distribution
- [x] Bounds checking (1-256 shards)

#### Phase 2.2: Per-Shard RaftNode - PARTIALLY COMPLETE

- [x] ShardedKeyValueStore implements KeyValueStore trait
- [x] Write routing to owner shard
- [x] Scan aggregation across all shards
- [x] Single-shard validation for batch/transaction ops
- [ ] Per-shard storage initialization (separate SQLite databases)
- [ ] Per-shard Raft cluster creation in bootstrap
- [ ] Integration tests with real RaftNode backends

#### Phase 2.3: Shard Membership - NOT STARTED

- [ ] ShardMembership manager
- [ ] Shard assignment on node join
- [ ] Shard failover handling
- [ ] Gossip integration for shard announcements

#### Phase 2.4: Rebalancing & Migration - NOT STARTED

- [ ] Key range migration
- [ ] Dual-write during migration
- [ ] Atomic cutover
- [ ] Chaos tests

### Integration Points Needed

1. **Bootstrap Integration** (`src/cluster/bootstrap.rs`)
   - Create N RaftNode instances per physical node
   - Each shard gets independent SQLite database
   - NodeId encoding: `physical_node_id | (shard_id << 32)`

2. **Network Layer** (`src/cluster/mod.rs`, `src/raft/network.rs`)
   - Shard-aware ALPN routing
   - Per-shard IrpcRaftNetworkFactory
   - Gossip announces shard membership

3. **Storage Layer** (`src/raft/storage_sqlite.rs`, `storage.rs`)
   - Per-shard database paths: `data/node-1/shard-{N}.db`
   - Per-shard Redb log: `data/node-1/shard-{N}-log.db`

### Success Criteria (Updated)

- [x] Consistent hash algorithm with uniform distribution
- [x] ShardRouter with O(1) key-to-shard lookup
- [x] ShardedKeyValueStore aggregates scans
- [ ] Per-shard Raft clusters (integration work)
- [ ] 10x data capacity vs single cluster
- [ ] Linear write throughput scaling
- [ ] Zero-downtime rebalancing

---

## P2: Layer Architecture

### Current State Analysis

**Existing Infrastructure:**

- Well-designed traits: `ClusterController`, `KeyValueStore`, `SqlQueryExecutor`
- Coordination primitives built on CAS (locks, elections, counters, queues)
- Log subscription via broadcast channel
- ALPN-based protocol routing

**Gap:**

- No secondary indexes beyond prefix scan
- No query planner
- No document/JSON layer
- No time series support

### Implementation Phases

#### Phase 3.1: Index Layer Foundation (Week 1-3)

- [ ] **Define IndexDefinition type**:

  ```rust
  pub struct IndexDefinition {
      name: String,
      key_extractor: Box<dyn Fn(&str, &str) -> Vec<String>>,
      unique: bool,
  }
  ```

- [ ] **Create IndexLayer wrapper**:

  ```rust
  pub struct IndexLayer {
      kv_store: Arc<dyn KeyValueStore>,
      indexes: Arc<RwLock<HashMap<String, IndexDefinition>>>,
  }
  ```

- [ ] **Implement index updates on write**
- [ ] **Add index scan API**
- [ ] **Add tests** for index consistency

#### Phase 3.2: Index Query Integration (Week 4-5)

- [ ] **Extend ScanRequest** with index hints
- [ ] **Implement index lookup path**
- [ ] **Add index statistics** for query planning
- [ ] **Benchmark** index vs full scan performance

#### Phase 3.3: Document Layer (Week 6-9)

- [ ] **Add JSON parsing/validation**
- [ ] **Create nested field index extractor**
- [ ] **Implement JSON path queries**
- [ ] **Add tests** for document operations

#### Phase 3.4: Time Series Layer (Week 10-12)

- [ ] **Design time-bucketed key strategy**
- [ ] **Implement time range queries**
- [ ] **Add aggregation functions** (sum, avg, min, max, count)
- [ ] **Add tests** for time series operations

### New Files Required

```
src/layers/
  mod.rs           (~100 lines)
  index.rs         (~800 lines)
  document.rs      (~600 lines)
  time_series.rs   (~500 lines)
```

### Success Criteria

- [ ] Secondary indexes update transactionally with writes
- [ ] Index scans are 10x+ faster than prefix scans for selective queries
- [ ] Document layer supports basic JSON queries
- [ ] Time series layer supports time range aggregations

---

## Timeline Summary (Parallel Execution)

**Track A: Transaction Model (P0)**

| Phase | Focus | Duration | Dependencies |
|-------|-------|----------|--------------|
| 1.1-1.2 | Version Tracking + Read/Write Sets | Weeks 1-4 | None |
| 1.3-1.4 | OCC Conflict Detection + Client API | Weeks 5-8 | Phase 1.1-1.2 |

**Track B: Horizontal Scaling (P1)**

| Phase | Focus | Duration | Dependencies |
|-------|-------|----------|--------------|
| 2.1-2.2 | Shard Router + Per-Shard Raft | Weeks 1-6 | None |
| 2.3-2.4 | Membership + Migration | Weeks 7-12 | Phase 2.1-2.2 |

**Track C: Layers (P2) - Starts after P0**

| Phase | Focus | Duration | Dependencies |
|-------|-------|----------|--------------|
| 3.1-3.2 | Index Layer | Weeks 9-13 | P0 complete |
| 3.3-3.4 | Document + Time Series | Weeks 14-18 | Phase 3.1-3.2 |

**Parallelization Strategy:**

- **Weeks 1-8**: P0 and P1 in parallel (2 developers or alternating focus)
- **Weeks 9-12**: P1 continues, P2 starts
- **Total timeline**: ~18 weeks with parallel tracks, ~12 weeks for core features

---

## Risk Assessment

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| SQLite write serialization limits OCC | Medium | High | Consider redb for multi-version storage |
| Sharding breaks existing scan semantics | High | Medium | Design parallel scan merge early |
| Index consistency during failures | Medium | High | Test with madsim chaos scenarios |
| Migration data loss | Low | Critical | Add checksums, dual-write validation |

---

## Progress Log

### 2025-12-22 (ULTRA MODE Analysis)

**Analysis Performed:**

- [x] 5 parallel subagents deployed for deep codebase analysis
- [x] FoundationDB OCC research (skiplist conflict detection, 5-second window)
- [x] Sharding pattern research (TiKV Multi-Raft, CockroachDB ranges)
- [x] Git history analysis of recent commits
- [x] Full codebase exploration of P0/P1 implementations

**Key Discoveries:**

1. Commit `98a88a67` added 1,999 lines implementing OCC + sharding foundation
2. TransactionBuilder client API fully implemented (284 lines)
3. SQLite state machine has complete OCC validation (lines 2610-2703)
4. ShardRouter + ShardedKeyValueStore ready for integration (1,116 lines)
5. In-memory test backend bypasses OCC validation (known gap)

**FoundationDB Research Insights:**

- Uses skiplist with 5-second version history for conflict detection
- Resolvers process conflict ranges in parallel (up to 16 concurrent)
- Client buffers writes locally for read-your-writes semantics
- Transaction retry with exponential backoff (max 1000ms)
- Conservative conflict detection (may reject non-conflicting txns)

**Sharding Research Insights:**

- Jump consistent hash (Aspen's choice) is optimal for KV stores
- TiKV uses Multi-Raft with per-region (shard) Raft groups
- CockroachDB uses range-based sharding with automatic splits
- Cross-shard transactions need 2PC or Percolator-style commit

### Previous Progress (2025-12-22)

- [x] Completed gap analysis vs FoundationDB and etcd
- [x] Explored transaction model implementation
- [x] Explored sharding architecture requirements
- [x] Explored layer architecture feasibility
- [x] Created initial progress tracking document

### Implementation Session (2025-12-22)

**Completed:**

- [x] **Phase 1: In-memory OCC validation** (`src/api/inmemory.rs`)
  - Added `VersionedValue` struct with version/create_revision/mod_revision tracking
  - Updated `DeterministicKeyValueStore` to use versioned storage
  - Implemented OCC read_set validation in OptimisticTransaction handler
  - All write operations now properly increment versions
  - read() and scan() return actual version values
  - 27/27 sharding tests pass

- [x] **Phase 2.1: NodeId encoding functions** (`src/sharding/mod.rs`)
  - Added `encode_shard_node_id(physical_node_id, shard_id) -> u64`
  - Added `decode_shard_node_id(encoded) -> (physical_node_id, shard_id)`
  - Added helper functions: `is_shard_node`, `get_shard_from_node_id`, `get_physical_node_id`
  - Backward compatible: shard_id=0 returns unmodified physical_node_id
  - 5 new unit tests added and passing

**Next Steps (Prioritized):**

1. **Phase 2.2**: Add per-shard storage paths in `storage_sqlite.rs` and `storage.rs`
2. **Phase 2.3**: Wire ShardedKeyValueStore to bootstrap in `cluster/bootstrap.rs`
3. **Phase 3**: Implement shard-aware ALPN routing in network layer
4. **Testing**: Add OCC conflict detection integration tests

---

## FoundationDB OCC Implementation Reference

Key patterns from FoundationDB that inform Aspen's implementation:

### Data Structures

```
Client maintains:
- Read Conflict Set: Vec<(key, read_version)>
- Write Conflict Set: Vec<(key, value)>
- Local write cache (tree structure for read-your-writes)

Resolver maintains:
- Version-augmented skiplist (last 5 seconds of committed writes)
- Operations: detectConflicts(), addConflictRanges(), removeBefore()
```

### Conflict Detection Algorithm

```
1. Proxy splits conflict ranges across resolvers by key range
2. Each resolver: "Has any key in read_set been written between read_version and commit_version?"
3. If ANY resolver detects conflict -> transaction aborts
4. If ALL resolvers return success -> transaction proceeds
```

### Aspen Implementation Mapping

| FoundationDB | Aspen Equivalent |
|--------------|------------------|
| Read Version | `version` column in state_machine_kv |
| Commit Version | Raft log index (mod_revision) |
| Resolver Skiplist | SQLite queries (could optimize to in-memory) |
| Client Write Buffer | TransactionBuilder write_set |
| onError() retry | Not implemented (client responsibility) |

### Performance Optimization Opportunities

1. **In-memory version cache**: Avoid SQLite query per conflict check
2. **Bounded version history**: Prune versions older than 5 seconds
3. **Batch version lookups**: Single query for multiple keys
4. **Consider redb**: Faster for append-only version tracking

---

## Sharding Architecture Reference

### Hash-Based vs Range-Based Sharding

| Approach | Pros | Cons | Used By |
|----------|------|------|---------|
| Jump Hash (Aspen) | O(1) routing, minimal redistribution | No range queries within shard | FoundationDB layers |
| Range-Based | Efficient range scans | Hot spots, complex splits | CockroachDB, TiKV |
| Virtual Nodes | Load balancing | Memory overhead | Cassandra, Riak |

### Multi-Raft Architecture (TiKV Pattern)

```
Physical Node 1:
  ├── Shard 0 RaftNode (voters: {1_0, 2_0, 3_0})
  ├── Shard 1 RaftNode (voters: {1_1, 2_1, 3_1})
  └── Shard 2 RaftNode (voters: {1_2, 2_2, 3_2})

Physical Node 2:
  ├── Shard 0 RaftNode (voters: {1_0, 2_0, 3_0})
  └── ... (may host different shards)
```

### Aspen Integration Plan

1. **Phase 1**: Single-shard mode works identically to current (backward compatible)
2. **Phase 2**: Multi-shard with independent Raft clusters per shard
3. **Phase 3**: Shard membership management + gossip integration
4. **Phase 4**: Zero-downtime rebalancing with dual-write migration
