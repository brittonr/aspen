# Kitty Cluster Test Comprehensive Fix Analysis
# Date: 2026-01-13
# Based on: Ultra mode analysis of 5 parallel test suites
# Overall Results: 91.5% pass rate (281 passed / 26 failed)

[summary]
title = "Kitty Cluster Test Suite Analysis and Fixes"
date = "2026-01-13"
test_results = { passed = 281, failed = 26, pass_rate = "91.5%" }

[test_suites]
kitty_cli_test = { passed = 121, failed = 12, total = 133, pass_rate = "91.0%" }
kitty_secrets_test = { passed = 117, failed = 3, total = 120, pass_rate = "97.5%" }
kitty_hooks_test = { passed = 23, failed = 7, total = 30, pass_rate = "76.7%" }
kitty_forge_test = { passed = 0, failed = 1, total = 1, pass_rate = "0%", note = "early abort" }
kitty_collab_test = { passed = 20, failed = 2, total = 22, pass_rate = "90.9%" }

# =============================================================================
# ISSUE 1: Hook Trigger Timeouts (7 failures)
# =============================================================================
[issues.hook_trigger_timeout]
description = "All hook trigger operations timeout after ~3-13 seconds"
symptoms = [
    "hook list and hook metrics work correctly",
    "hook trigger consistently times out",
    "All 7 failures in kitty-hooks-test are trigger operations"
]
root_cause = """
The ForwardHandler in crates/aspen-hooks/src/handlers.rs calls client.shutdown().await
without a timeout at line 436. The Iroh endpoint.close() can hang for 10+ seconds
on network issues, blocking the entire RPC handler.

The shutdown happens AFTER the RPC completes successfully, so the client sees
the request timeout even though the operation actually succeeded on the target.
"""
fix_location = "crates/aspen-hooks/src/handlers.rs:436"
fix_description = "Added 2-second timeout to client.shutdown() call"
fix_status = "COMPLETED"
code_change = """
Before: client.shutdown().await;
After:  let _ = timeout(Duration::from_secs(2), client.shutdown()).await;
"""

# =============================================================================
# ISSUE 2: Cluster Initialization Race (1 failure)
# =============================================================================
[issues.cluster_init_race]
description = "Forge test git init failed with NOT_INITIALIZED"
symptoms = [
    "First Forge test fails immediately",
    "Later tests (that started after more time) succeed",
    "Subsystem readiness check (git list) passed but subsequent git init failed"
]
root_cause = """
The cluster reports ready when Raft membership is established, but individual
nodes may still be in a transient state. The ticket-based connection can route
to any node, and if it hits a node that hasn't fully synced, NOT_INITIALIZED
is returned.

The retry logic (3 retries, 1s+2s+4s = 7s total) was not sufficient for some
timing scenarios.
"""
fix_location = "scripts/kitty-forge-test.sh:350-384"
fix_description = """
Improved retry logic for transient distributed system errors:
- Increased max_retries from 3 to 5
- Increased initial retry_delay from 1s to 2s
- Added FORGE_UNAVAILABLE to retry patterns
- Capped backoff at 16s to avoid excessive waits
- Total retry window: 2+4+8+16+16 = 46s max
"""
fix_status = "COMPLETED"

# =============================================================================
# ISSUE 3: PKI Custom Mount Failures (3 failures)
# =============================================================================
[issues.pki_custom_mount]
description = "PKI operations fail when using custom mount points"
symptoms = [
    "Default mount ('pki') works fine (117/120 secrets tests pass)",
    "Custom mounts (e.g., 'pki-prod', 'custom-pki') fail",
    "Error message incorrectly reports 'pki' mount even for custom mounts"
]
root_cause_immediate = """
The DefaultPkiStore hardcoded 'pki' in error messages at three locations:
- store.rs:322 (generate_root)
- store.rs:419 (generate_intermediate)
- store.rs:463 (set_signed_intermediate)

This causes confusing error messages that don't reflect the actual mount.
"""
root_cause_architectural = """
The SecretsService only has a single pki_store instance, and the RPC handlers
ignore the mount parameter from requests. The architecture was designed for
single-mount usage but the API accepts mount parameters.

True multi-mount support would require:
1. Mount -> Store registry in SecretsService
2. Dynamic store creation for new mounts
3. Mount parameter threading through all handler functions
"""
fix_immediate_location = "crates/aspen-secrets/src/pki/store.rs"
fix_immediate_description = """
- Added 'mount' field to DefaultPkiStore struct
- Added with_mount() constructor for custom mount points
- Changed hardcoded 'pki' errors to use self.mount.clone()
"""
fix_immediate_status = "COMPLETED"
fix_architectural_status = "DEFERRED - Requires significant refactoring"

# =============================================================================
# ISSUE 4: Patch Merge Failure (2 failures)
# =============================================================================
[issues.patch_merge_failure]
description = "Patch merge operation fails silently"
symptoms = [
    "Patch creation and listing works",
    "Merge operation returns successfully (hash returned)",
    "Merged state not reflected when resolving patch"
]
root_cause = """
The MergeHeads operation is stored in the DAG but completely ignored during
state reconstruction. In crates/aspen-forge/src/cob/patch.rs:324-327:

    CobOperation::MergeHeads { .. } => {
        // Merge commits don't directly modify patch state
    }

The code comment says "Merge operations are handled at the store level during
resolution" but this is NOT implemented anywhere. The FieldResolution data
stored with MergeHeads operations is never applied.

This is a design gap - the merge semantics were specified but not fully
implemented. The topological sort orders changes correctly, but there's no
logic to select which change's value to use for conflicting scalar fields.
"""
fix_status = "NOT FIXED - Design gap requires careful implementation"
design_requirements = [
    "Detect MergeHeads operations during resolve_patch()",
    "Track field resolutions from each MergeHeads operation",
    "Apply field resolutions to override scalar values",
    "Handle nested merges (merge of merges)",
    "Consider performance implications for large DAGs"
]

# =============================================================================
# FILES MODIFIED
# =============================================================================
[files_modified]
handlers_rs = "crates/aspen-hooks/src/handlers.rs"
pki_store_rs = "crates/aspen-secrets/src/pki/store.rs"
forge_test_sh = "scripts/kitty-forge-test.sh"

# =============================================================================
# VERIFICATION STATUS
# =============================================================================
[verification]
compile_check = "PENDING"
test_run = "PENDING"
note = "Fixes have been applied but not yet verified with full test run"

# =============================================================================
# RECOMMENDATIONS
# =============================================================================
[recommendations]
immediate = [
    "Run nix develop -c cargo build to verify compilation",
    "Run the kitty test suites again to verify fixes",
    "Monitor hook trigger tests for timeout improvements"
]
follow_up = [
    "Implement proper multi-mount support for secrets engines",
    "Implement MergeHeads field resolution in COB state reconstruction",
    "Add integration tests for merge conflict resolution",
    "Consider adding per-node health checks before subsystem operations"
]
