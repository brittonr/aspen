ASPEN NETWORK AND P2P LAYER AUDIT REPORT
================================================================================

EXECUTIVE SUMMARY
================================================================================

This audit examines the network and P2P layer in Aspen, focusing on Iroh
integration, transport layer, protocol handling, and partition tolerance.

AUDIT FINDINGS: CRITICAL AND HIGH-PRIORITY ISSUES IDENTIFIED

================================================================================
1. IROH INTEGRATION
================================================================================

1.1 ISSUE: Missing Connection Lifecycle Management
────────────────────────────────────────────────────────────────────────────
Location: src/raft/network.rs (lines 149-182)
Severity: HIGH

Description:
The RPC client opens a new connection to every peer for every RPC request.
There is NO connection pooling, caching, or reuse. Each append_entries, vote,
or snapshot RPC creates a fresh Iroh connection.

Code:
  149:  let connection_result = tokio::time::timeout(
  150:      IROH_CONNECT_TIMEOUT,
  151:      endpoint.connect(peer_addr.clone(), b"raft-rpc"),
  152:  )

Impact:
- Resource exhaustion: 1000s of concurrent short-lived connections
- Excessive context switching and connection setup overhead
- Violates Tiger Style principle of bounded resource allocation
- Production deployments will struggle with large clusters
- Each connection attempt includes full TLS/QUIC handshake

Recommendation:
Implement connection pooling with bounded per-peer connection limits
(e.g., MAX_CONNECTIONS_PER_PEER), connection reuse, and TTL-based cleanup.


1.2 ISSUE: No Explicit Endpoint Lifecycle Documentation
────────────────────────────────────────────────────────────────────────────
Location: src/cluster/mod.rs (lines 634-642)
Severity: MEDIUM

Description:
The shutdown path comments indicate that gossip and router cleanup happens
automatically when endpoint closes, but there's no explicit testing of this
cleanup in edge cases (partial failures, shutdown races).

Code:
  634:  /// Tiger Style: Explicit cleanup with bounded wait time.
  635:  pub async fn shutdown(&self) -> Result<()> {
  636:      // Note: Gossip and router are owned by the endpoint and will be
  637:      // cleaned up when the endpoint closes. No explicit shutdown needed.

Impact:
- Undefined behavior if endpoint.close() hangs
- No timeout on endpoint closure (gossip/router cleanup unbounded)
- Potential resource leaks on shutdown errors
- Doesn't follow Tiger Style principle of explicit timeouts

Recommendation:
Add explicit timeout wrapper around endpoint.close() matching bootstrap
timeouts. Document gossip/router cleanup expectations.


================================================================================
2. TRANSPORT LAYER - CRITICAL ISSUES
================================================================================

2.1 ISSUE: No Message Framing or Length Prefix Protocol
────────────────────────────────────────────────────────────────────────────
Location: src/raft/network.rs (lines 173-182) and src/raft/server.rs (lines 150-158)
Severity: CRITICAL

Description:
The transport layer sends raw postcard-serialized bytes with NO message
framing, length prefix, or message boundary markers.

Client sends:
  174:  let serialized = postcard::to_stdvec(&request)...
  176:  send_stream.write_all(&serialized)
  181:  send_stream.finish()

Server receives:
  151:  let buffer = recv.read_to_end(MAX_RPC_MESSAGE_SIZE as usize)

Issues:
1. read_to_end() requires stream closure to detect message boundary
   (finish() on client side)
2. No way to send multiple messages on same stream
3. If client sends partial message then crashes, server hangs indefinitely
4. No message size validation BEFORE reading
5. No length prefix means server must buffer entire message before
   knowing if it's complete

Attack/Failure Scenarios:
- Malicious peer sends 10 MB message, claiming MAX_RPC_MESSAGE_SIZE (10 MB)
  Server allocates 10 MB even if actual valid message is 100 bytes
- Network corruption adds garbage after valid message → parse error
- Peer sends message in multiple writes without finishing stream
  → Server hangs waiting for stream close
- No protection against slow-loris: attacker sends 1 byte per hour
  over 10 MB threshold, tying up resources

Code Flow Problems:
  - read_to_end(limit) reads until stream EOF, not until message complete
  - No way to detect "truncated but valid" vs "incomplete" message
  - Single large field in message can consume entire allocation

Impact:
- Resource exhaustion (memory allocation attacks)
- Protocol deadlocks on stream closure
- No replay detection mechanism
- No message ordering guarantee

Recommendation: CRITICAL
Implement proper message framing with:
1. 4-byte big-endian length prefix (u32) before each message
2. Read length prefix first, validate against MAX_RPC_MESSAGE_SIZE
3. Then read exactly that many bytes
4. Remove reliance on stream.finish() for message boundary detection
5. Add per-connection message count limit to prevent flooding
6. Implement request ID + nonce for replay protection


2.2 ISSUE: Unbounded Stream Resource Allocation
────────────────────────────────────────────────────────────────────────────
Location: src/raft/server.rs (lines 122-139)
Severity: HIGH

Description:
Server accepts unlimited bidirectional streams from a single connection
without any per-connection or global limits.

Code:
  122:  loop {
  123:      let stream = match connection.accept_bi().await {
  124:          Ok(stream) => stream,
  125:          Err(err) => {
  126:              debug!(remote_node = %remote_node_id, error = %err, "connection closed");
  127:              break;
  128:          }
  129:      };
  131:      let raft_core_clone = raft_core.clone();
  132:      let (send, recv) = stream;
  133:      tokio::spawn(async move {
  134:          if let Err(err) = handle_rpc_stream((recv, send), raft_core_clone).await {
  135:              error!(error = %err, "failed to handle RPC stream");
  136:          }
  137:      });

Issues:
1. No limit on concurrent streams per connection
2. No backpressure mechanism if task queue grows
3. Each stream spawns unbounded tokio task
4. Malicious peer can open 1000+ streams, causing task explosion
5. No timeout on stream setup or initial message arrival

Attack Scenario:
- Malicious peer: open_bi() 10,000 times (Iroh allows this)
- Server spawns 10,000 tasks, each waiting for read_to_end()
- All tasks are alive, consuming stack space and scheduler time
- Even if peer never sends data, tasks block indefinitely on read

Impact:
- DoS via stream explosion
- Memory exhaustion from task overhead
- Scheduler starvation
- Tasks hang if peer sends incomplete message

Recommendation: CRITICAL
1. Add per-connection stream limit (e.g., 100 max concurrent)
2. Implement backpressure: reject new streams if queue full
3. Add timeout on stream.read() to detect stalled peers
4. Track active streams per remote_node_id and enforce limit


2.3 ISSUE: No Per-Connection Read Timeout
────────────────────────────────────────────────────────────────────────────
Location: src/raft/server.rs (lines 146-154)
Severity: HIGH

Description:
The handle_rpc_stream function calls recv.read_to_end() with NO timeout.
If peer sends partial message and goes silent, server task hangs forever.

Code:
  151:  let buffer = recv
  152:      .read_to_end(MAX_RPC_MESSAGE_SIZE as usize)
  153:      .await
  154:      .context("failed to read RPC message")?;

Issues:
1. No timeout between message arrival and completion
2. Peer can stall indefinitely after sending 1 byte
3. Task never completes, accumulates in process
4. Multiplied by stream explosion (issue 2.2), causes DoS

Recommended Fix: Add timeout (matching IROH_READ_TIMEOUT):
  tokio::time::timeout(
      IROH_READ_TIMEOUT,
      recv.read_to_end(MAX_RPC_MESSAGE_SIZE)
  ).await?


================================================================================
3. PROTOCOL HANDLING ISSUES
================================================================================

3.1 ISSUE: No Request/Response Matching or Request ID
────────────────────────────────────────────────────────────────────────────
Location: src/raft/rpc.rs, src/raft/network.rs
Severity: MEDIUM

Description:
Request/response matching relies entirely on:
1. Synchronous send then read on same stream
2. No explicit request ID in protocol

This means:
- If stream gets corrupted or closed early, no way to match response to request
- No support for pipelined requests (must wait for response before next request)
- Cannot detect "response to old request" scenarios

Code Flow (network.rs lines 166-195):
  1. Open stream
  2. Send request
  3. Close send side
  4. Read response
  5. Deserialize response
  6. Hope response type matches request type (line 258: "unexpected response type")

Response Validation:
  257:  match response {
  258:      RaftRpcResponse::AppendEntries(result) => Ok(result),
  259:      _ => Err(RPCError::Network(...))

Issues:
- No explicit request/response correlation
- If peer returns wrong response type, error is generic
- No replay detection (old response to new request)
- No nonce/sequence number

Impact:
- Silent data corruption if message ordering violated
- Cannot implement pipelining for performance
- Weak replay attack protection

Recommendation:
Add request ID (u32 or u64) to protocol:
  struct RaftRpcRequest {
      request_id: u64,
      request: RaftRpcProtocol,
  }
  struct RaftRpcResponse {
      request_id: u64,
      response: RaftRpcResponseBody,
  }


3.2 ISSUE: Message Authentication Missing (Gossip)
────────────────────────────────────────────────────────────────────────────
Location: src/cluster/gossip_discovery.rs (lines 209-238)
Severity: MEDIUM

Description:
Gossip announcements are broadcast but NOT cryptographically signed.
Comments mention SecretKey but it's never used for message signing.

Code:
  171:  let announcement = match PeerAnnouncement::new(...)
  210:  match PeerAnnouncement::from_bytes(&msg.content) {
  219:  tracing::debug!("received peer announcement from node_id={}")

PeerAnnouncement struct (lines 67-89):
  #[derive(Debug, Clone, Serialize, Deserialize)]
  struct PeerAnnouncement {
      node_id: NodeId,
      endpoint_addr: EndpointAddr,
      timestamp_micros: u64,
      // ❌ NO SIGNATURE FIELD
  }

Issues:
1. Malicious peer can broadcast fake announcements claiming to be node 1
2. Man-in-the-middle can modify announcements
3. Timestamp is not signed, can be replayed
4. No way to verify sender identity

Attack Scenario:
- Attacker joins gossip topic, broadcasts:
  {node_id: 1, endpoint_addr: attacker_ip:1234, timestamp: now}
- Legitimate node 1 goes offline
- All nodes connect to attacker instead of legitimate node 1
- Attacker intercepts Raft RPCs or performs Byzantine attacks

Impact:
- Impersonation attacks
- Man-in-the-middle attacks
- Cluster can be poisoned by false peer announcements

Comment in code (lines 13-16):
  "# Security
   Messages are signed with the node's SecretKey and verified on receipt.
   Invalid signatures are rejected (fail-fast)."

BUT: No signature in code! Comments are outdated/aspirational.

Recommendation: CRITICAL
1. Add signature field to PeerAnnouncement
2. Sign with IrohEndpointManager.secret_key()
3. Verify sender's public key matches announced endpoint_id
4. Include nonce or request ID to prevent simple replays
5. Use established crypto (ed25519 in iroh)


3.3 ISSUE: No Idempotency Tracking for Raft Operations
────────────────────────────────────────────────────────────────────────────
Location: src/raft/network.rs
Severity: MEDIUM

Description:
Raft itself handles idempotency via log indices, but network layer
doesn't track request IDs. If client retries due to timeout, server
can't distinguish:
- Duplicate of old request (should be ignored)
- New request with same payload (should process)

For Raft state machine operations (KV store), this is critical.

Impact:
- Unclear if duplicate AppendEntries is actually duplicate or new election
- Client can't safely retry without risk of double-application
- No "request in flight" tracking

Recommendation:
Implement request ID in RPC protocol and track in-flight requests per peer.


================================================================================
4. PARTITION HANDLING & SPLIT-BRAIN ISSUES
================================================================================

4.1 ISSUE: No Explicit Network Partition Detection
────────────────────────────────────────────────────────────────────────────
Location: src/raft/node_failure_detection.rs (lines 15-28, 63-185)
Severity: HIGH

Description:
The NodeFailureDetector classifies failures as:
- Healthy: Raft OK
- ActorCrash: Raft down, Iroh up
- NodeCrash: Both down

But this is NOT the same as network partition detection.

Scenario: Network partition (node unreachable):
1. All Raft RPCs fail (ConnectionStatus::Disconnected)
2. Iroh connection also fails (ConnectionStatus::Disconnected)
3. Detector classifies as NodeCrash → operator notified

But actually:
- Node might be fine, just partitioned
- Operator might promote learner, creating two leaders (split-brain)
- When partition heals, we have inconsistent state

Code (lines 171-185):
  pub fn classify_failure(
      &self,
      raft_heartbeat: ConnectionStatus,
      iroh_connection: ConnectionStatus,
  ) -> FailureType {
      match (raft_heartbeat, iroh_connection) {
          (ConnectionStatus::Connected, _) => FailureType::Healthy,
          (ConnectionStatus::Disconnected, ConnectionStatus::Connected) => {
              FailureType::ActorCrash
          }
          (ConnectionStatus::Disconnected, ConnectionStatus::Disconnected) => {
              FailureType::NodeCrash  // ❌ Could be partition!
          }
      }
  }

Impact:
- Cannot distinguish partition from crash
- Operator can't make informed decisions
- Risk of accidental split-brain if manual recovery triggered
- No automatic partition-aware quorum adjustment

Recommendation:
Add partition detection alongside failure detection:
1. Track time since last successful contact (in addition to connection status)
2. Add "PartitionDetected" state distinct from "NodeCrash"
3. Prevent learner promotion if node unreachable for <T seconds (e.g., 30s)
   instead of just checking failure detector status
4. Document operator decision tree for each failure type


4.2 ISSUE: No Quorum Verification During Membership Changes
────────────────────────────────────────────────────────────────────────────
Location: src/raft/learner_promotion.rs (lines 59-92)
Severity: HIGH

Description:
The learner promotion safety checks verify:
- Learner is healthy
- Learner is caught up
- Cooldown period elapsed
- Max voters not exceeded

But it does NOT verify:
- Current cluster still has quorum
- Promotion would maintain quorum

Code:
  59:  /// Enforces safety checks and cooldown periods to prevent unsafe membership
  60:  /// changes. Uses the NodeFailureDetector to distinguish between actor crashes
  61:  /// and node crashes.

  PromotionError::NoQuorumWithoutFailedNode { failed_node_id: Option<NodeId> }

  This error suggests quorum checking exists, but...

Looking at learner_promotion.rs more carefully:
The error type exists but actual quorum verification logic is not shown
in the limited read. Need to check if it's actually enforced.

Impact:
- Could promote learner when cluster has lost quorum
- If minority partition promotes a learner, they become a separate consensus group
- Split-brain risk

Recommendation:
Ensure promotion explicitly checks:
  if new_voter_count > current_quorum_size:
      verify current cluster still has quorum of existing voters


4.3 ISSUE: No Heartbeat/Liveness During Partitions
────────────────────────────────────────────────────────────────────────────
Location: src/raft/network.rs, src/raft/constants.rs
Severity: MEDIUM

Description:
Heartbeat timeout is configured via Raft config (default from openraft).
If leader becomes partitioned from followers:

1. Leader can't reach followers → network errors recorded
2. Followers don't receive heartbeats → election timeout fires
3. Minority partition elects new leader
4. When partition heals, we have two leaders (split-brain)

The code tracks "Disconnected" status but doesn't have explicit
"partition depth" awareness. A leader in a partition with only 1 node
will keep trying to replicate logs indefinitely.

Impact:
- Partitioned minority can become new leader
- Both partitions accept writes (Byzantine behavior)
- Data loss/corruption on merger

Recommendation:
1. Implement leadership check against quorum in partition-aware way
2. Leader should step down if can't reach quorum of followers
3. Followers should only elect new leader if quorum reachable


================================================================================
5. CONNECTION POOLING & RESOURCE EXHAUSTION POTENTIAL
================================================================================

5.1 ISSUE: Stream Resource Leak on Deserialization Failure
────────────────────────────────────────────────────────────────────────────
Location: src/raft/server.rs (lines 146-158)
Severity: MEDIUM

Description:
If deserialization fails, the tokio task exits, but:
1. Stream is dropped (should be OK via QUIC reset)
2. But if MAX_RPC_MESSAGE_SIZE is reached, entire buffer was allocated
3. No backpressure before deserialization

Code:
  151:  let buffer = recv.read_to_end(MAX_RPC_MESSAGE_SIZE as usize).await?;
  158:  let request: RaftRpcProtocol = postcard::from_bytes(&buffer)?;

  If from_bytes fails, error is returned and task exits.

Impact:
- Failed deserialization still consumed network bandwidth
- Buffer allocated even for garbage data
- Multiplied by stream explosion risk

Recommendation:
Implement message framing with length prefix (see issue 2.1).


5.2 ISSUE: Per-Connection Memory Unbounded
────────────────────────────────────────────────────────────────────────────
Location: src/raft/server.rs
Severity: MEDIUM

Description:
Each connection can have unlimited tasks reading from unlimited streams.
Each task can allocate up to MAX_RPC_MESSAGE_SIZE = 10 MB.

Example attack:
- Open 100 streams per connection
- Each stream reads 10 MB on slow-loris pattern
- 100 connections × 100 streams × 10 MB = 100 GB memory
- Not bounded by any per-connection limit

Impact:
- Memory exhaustion DoS
- No per-connection resource limits
- No global connection count limit

Recommendation:
1. Add per-connection concurrent stream limit (e.g., 10)
2. Add per-node total connection count limit (e.g., 5)
3. Add per-connection memory accounting
4. Reject new streams if backpressure detected


================================================================================
6. GOSSIP DISCOVERY ISSUES
================================================================================

6.1 ISSUE: No Peer Verification Before Adding to Network Factory
────────────────────────────────────────────────────────────────────────────
Location: src/cluster/gossip_discovery.rs (lines 224-238)
Severity: HIGH

Description:
When an announcement is received:

Code:
  224:  if let Some(ref factory) = receiver_network_factory {
  225:      factory
  226:          .add_peer(
  227:              announcement.node_id,
  228:              announcement.endpoint_addr.clone(),
  229:          )
  230:          .await;

Issues:
1. No verification that node_id matches endpoint_addr ownership
2. No DNS/Pkarr verification
3. No repeated announcement detection (could spam network factory)
4. No TTL on peer entries

Scenario:
- Attacker broadcasts {node_id: 5, endpoint_addr: attacker_ip}
- Legitimate node 5 is somewhere else
- All nodes add attacker to network factory
- Raft RPCs go to attacker

Impact:
- Eclipse attack: all traffic can be redirected to attacker
- Man-in-the-middle on Raft RPC
- Impersonation of cluster members

Recommendation:
1. Verify endpoint_id signature on announcement (see issue 3.2)
2. Add rate limiting per source node (max 1 announcement per 60s)
3. Implement peer table TTL (e.g., 5 minutes)
4. Log peer additions and flag rapid changes


6.2 ISSUE: No Lagged Event Handling
────────────────────────────────────────────────────────────────────────────
Location: src/cluster/gossip_discovery.rs (lines 262-264)
Severity: MEDIUM

Description:
Gossip event handling:

Code:
  262:  Some(Ok(Event::Lagged)) => {
  263:      tracing::warn!("gossip receiver lagged, messages may be lost");

If gossip is lagged and we miss peer announcements:
- New peers won't be discovered
- Doesn't trigger re-sync or peer rediscovery
- Just logs warning

Impact:
- Silent peer discovery failures
- Cluster can get partitioned silently
- No automatic recovery

Recommendation:
On Event::Lagged, trigger peer re-discovery or re-request bootstrap info.


================================================================================
7. TIMEOUT CONFIGURATION ISSUES
================================================================================

7.1 ISSUE: IROH_READ_TIMEOUT (10s) Too Long for Heartbeats
────────────────────────────────────────────────────────────────────────────
Location: src/raft/constants.rs (lines 43-51)
Severity: MEDIUM

Description:
IROH_READ_TIMEOUT = 10 seconds is appropriate for snapshot transfer but
too long for heartbeat RPCs (which should complete in milliseconds).

Code:
  185:  let response_buf = tokio::time::timeout(
  186:      IROH_READ_TIMEOUT,
  187:      recv_stream.read_to_end(MAX_RPC_MESSAGE_SIZE as usize),
  188:  ).await

This timeout applies to all RPC types (append_entries, vote, snapshots).

Impact:
- Heartbeat latency = 10 second worst case
- Election timeout might not trigger in timely fashion
- Network partition detection delayed by 10s

Recommendation:
Use different timeouts per RPC type:
- HEARTBEAT_TIMEOUT: 2 seconds
- VOTE_TIMEOUT: 5 seconds
- SNAPSHOT_TIMEOUT: 60 seconds


================================================================================
8. METADATA & PEER DISCOVERY ISSUES
================================================================================

8.1 ISSUE: No Signature Verification in Endpoint Registration
────────────────────────────────────────────────────────────────────────────
Location: src/cluster/metadata.rs
Severity: MEDIUM

Description:
Nodes register themselves in metadata store with:
- node_id
- endpoint_id (as hex string)
- raft_addr

But endpoint_id is never verified to belong to this node. A compromised
node could register false endpoint_id, and clients would connect there.

Impact:
- Rogue node can claim to be another node's endpoint
- Registry poisoning

Recommendation:
Require node to prove possession of endpoint secret key before registry update.


================================================================================
SUMMARY TABLE
================================================================================

CRITICAL (Requires immediate fix):
├─ 2.1: No message framing/length prefix (security + stability)
├─ 2.2: Unbounded stream allocation (DoS via stream explosion)
├─ 3.2: Missing message authentication for gossip (impersonation)
├─ 4.1: No partition detection distinct from crash
└─ 6.1: No peer verification before network factory add (eclipse attack)

HIGH (Required for production):
├─ 1.1: No connection pooling (resource exhaustion)
├─ 2.3: No per-connection read timeout (indefinite hang)
├─ 4.2: Potential quorum bypass in learner promotion
└─ 7.1: Timeout configuration issues

MEDIUM (Should fix before production):
├─ 1.2: No endpoint shutdown timeout
├─ 3.1: No request/response matching
├─ 3.3: No idempotency tracking
├─ 4.3: No heartbeat liveness during partitions
├─ 5.1: Stream resource leak on deser failure
├─ 5.2: Per-connection memory unbounded
├─ 6.2: No lagged event handling
└─ 8.1: No signature verification in registry

================================================================================
RECOMMENDATIONS SUMMARY
================================================================================

PHASE 1 (CRITICAL - Fix before production):
1. Implement message framing with length prefix
2. Add per-connection and per-stream limits
3. Add message timeout to server RPC handler
4. Implement cryptographic signing for gossip announcements
5. Add request ID to RPC protocol for proper matching

PHASE 2 (HIGH - Fix before ≥3 nodes):
6. Implement connection pooling with per-peer limits
7. Add partition detection distinct from node crash
8. Verify quorum implications of learner promotion
9. Implement heartbeat liveness checks

PHASE 3 (MEDIUM - Fix for hardening):
10. Add endpoint secret key verification for node registration
11. Implement peer entry TTL and deduplication
12. Add per-RPC type timeout configuration
13. Implement replay detection with nonce/sequence numbers

================================================================================
